{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25cddd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 1234) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98f7661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nominal values\n",
    "\n",
    "adam_epochs = 45000\n",
    "adam_lr = 1e-3\n",
    "lbfgs_epochs = 50000\n",
    "L_n = 7.25*1e-4\n",
    "RL_n = 0.314\n",
    "C_n = 1.645 *1e-4\n",
    "RC_n = 2.01 * 1e-1\n",
    "Rdson_n = 0.221\n",
    "Rload1_n = 3.1\n",
    "Rload2_n = 10.2\n",
    "Rload3_n = 6.1\n",
    "Vin_n = 48\n",
    "VF_n = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c5e35",
   "metadata": {},
   "source": [
    "## Let's Test The Accuracy of dt of Runge-Kutta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13324ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without sub-steps:\n",
      "i1_pred = 8.805077, i1 = 8.805084, Delta_i = -6.498587e-06\n",
      "v1_pred = 26.041748, v1 = 26.041720, Delta_v = 2.817933e-05\n",
      "With 1000 sub-steps:\n",
      "i1_pred = 8.805077, i1 = 8.805084, Delta_i = -6.574731e-06\n",
      "v1_pred = 26.041749, v1 = 26.041720, Delta_v = 2.886677e-05\n"
     ]
    }
   ],
   "source": [
    "def predict_next_state(\n",
    "    i0: float,\n",
    "    v0: float,\n",
    "    D: float,\n",
    "    dt: float,\n",
    "    L: float,\n",
    "    RL: float,\n",
    "    C: float,\n",
    "    RC: float,\n",
    "    Rdson: float,\n",
    "    Rload: float,\n",
    "    Vin: float,\n",
    "    Vf: float,\n",
    "    n_substeps: int = 100,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Predict next state using RK4 with optional sub-stepping.\"\"\"\n",
    "    h = dt / n_substeps  # smaller time step\n",
    "    i, v = i0, v0\n",
    "\n",
    "    def f(i, v):\n",
    "        di = (-(RL + Rdson * D) * i - v + D * Vin - (1 - D) * Vf) / L\n",
    "        dv = (Rload * i - v + C * RC * Rload * di) / (C * (RC + Rload))\n",
    "        return di, dv\n",
    "\n",
    "    for _ in range(n_substeps):\n",
    "        k1_i, k1_v = f(i, v)\n",
    "        k2_i, k2_v = f(i + 0.5 * h * k1_i, v + 0.5 * h * k1_v)\n",
    "        k3_i, k3_v = f(i + 0.5 * h * k2_i, v + 0.5 * h * k2_v)\n",
    "        k4_i, k4_v = f(i + h * k3_i, v + h * k3_v)\n",
    "\n",
    "        i += (h / 6) * (k1_i + 2 * k2_i + 2 * k3_i + k4_i)\n",
    "        v += (h / 6) * (k1_v + 2 * k2_v + 2 * k3_v + k4_v)\n",
    "\n",
    "    return i, v\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pinn_buck.io import TransientData\n",
    "\n",
    "# Load data\n",
    "db_dir = Path(r\"C:\\Users\\JC28LS\\OneDrive - Aalborg Universitet\\Desktop\\Work\\Databases\")\n",
    "db_name = \"buck_converter_Shuai_processed.h5\"\n",
    "tr1 = TransientData.from_h5(db_dir / db_name, \"ideal\", 1)\n",
    "\n",
    "# Index to test\n",
    "idx = 51\n",
    "i0 = tr1.i[idx]\n",
    "v0 = tr1.v[idx]\n",
    "i1 = tr1.i[idx + 1]\n",
    "v1 = tr1.v[idx + 1]\n",
    "D = tr1.D[idx]\n",
    "dt = tr1.dt[idx]\n",
    "\n",
    "# Nominal parameters\n",
    "i1_pred_ssteps, v1_pred_ssteps = predict_next_state(\n",
    "    i0=i0,\n",
    "    v0=v0,\n",
    "    D=D,\n",
    "    dt=dt,\n",
    "    L=7.25e-4,\n",
    "    RL=0.314,\n",
    "    C=1.645e-4,\n",
    "    RC=0.201,\n",
    "    Rdson=0.221,\n",
    "    Rload=3.1,\n",
    "    Vin=48.0,\n",
    "    Vf=1.0,\n",
    "    n_substeps=1000,\n",
    ")\n",
    "\n",
    "# Predict without sub-steps\n",
    "i1_pred, v1_pred = predict_next_state(\n",
    "    i0=i0,\n",
    "    v0=v0,\n",
    "    D=D,\n",
    "    dt=dt,\n",
    "    L=L_n,\n",
    "    RL=RL_n,\n",
    "    C=C_n,\n",
    "    RC=RC_n,\n",
    "    Rdson=Rdson_n,\n",
    "    Rload=Rload1_n,  # Using Rload1 as an example\n",
    "    Vin=Vin_n,\n",
    "    Vf=VF_n,\n",
    "    n_substeps=1,  # No sub-steps\n",
    ")\n",
    "\n",
    "print(\"Without sub-steps:\")\n",
    "print(f\"i1_pred = {i1_pred:.6f}, i1 = {i1:.6f}, Delta_i = {i1_pred - i1:.6e}\")\n",
    "print(f\"v1_pred = {v1_pred:.6f}, v1 = {v1:.6f}, Delta_v = {v1_pred - v1:.6e}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"With 1000 sub-steps:\")\n",
    "print(f\"i1_pred = {i1_pred_ssteps:.6f}, i1 = {i1:.6f}, Delta_i = {i1_pred_ssteps - i1:.6e}\")\n",
    "print(f\"v1_pred = {v1_pred_ssteps:.6f}, v1 = {v1:.6f}, Delta_v = {v1_pred_ssteps - v1:.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d838bcb",
   "metadata": {},
   "source": [
    "**There is virtually no difference in the prediction error!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92258ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the datasets\n",
    "from h5_auxiliaries.datatransients import TransientData\n",
    "\n",
    "db_dir = Path(r\"C:\\Users\\JC28LS\\OneDrive - Aalborg Universitet\\Desktop\\Work\\Databases\")\n",
    "db_name = \"buck_converter_Shuai_processed.h5\"\n",
    "\n",
    "tr1 = TransientData.from_h5(db_dir / db_name, \"ideal\", 1)\n",
    "tr2 = TransientData.from_h5(db_dir / db_name, \"ideal\", 2)\n",
    "tr3 = TransientData.from_h5(db_dir / db_name, \"ideal\", 3)\n",
    "\n",
    "i1_n = tr1.i[:-1]\n",
    "v1_n = tr1.v[:-1]\n",
    "i1_np1 = tr1.i[1:]\n",
    "v1_np1 = tr1.v[1:]\n",
    "D1 = tr1.D[:-1]\n",
    "dt1 = tr1.dt[:-1]\n",
    "\n",
    "i2_n = tr2.i[:-1]\n",
    "v2_n = tr2.v[:-1]\n",
    "i2_np1 = tr2.i[1:]\n",
    "v2_np1 = tr2.v[1:]\n",
    "D2 = tr2.D[:-1]\n",
    "dt2 = tr2.dt[:-1]\n",
    "\n",
    "\n",
    "i3_n = tr3.i[:-1]\n",
    "v3_n = tr3.v[:-1]\n",
    "i3_np1 = tr3.i[1:]\n",
    "v3_np1 = tr3.v[1:]\n",
    "D3 = tr3.D[:-1]\n",
    "dt3 = tr3.dt[:-1]\n",
    "\n",
    "\n",
    "\n",
    "i_n = np.concatenate((i1_n, i2_n, i3_n), axis=0)\n",
    "i_np1 = np.concatenate((i1_np1, i2_np1, i3_np1), axis=0)\n",
    "v_n = np.concatenate((v1_n, v2_n, v3_n), axis=0)\n",
    "v_np1 = np.concatenate((v1_np1, v2_np1, v3_np1), axis=0)\n",
    "D = np.concatenate((D1, D2, D3), axis=0)\n",
    "dt = np.concatenate((dt1, dt2, dt3), axis=0)\n",
    "\n",
    "\n",
    "X = np.stack([i_n, v_n, i_np1, v_np1, D, dt], axis=1)\n",
    "Rload = np.concatenate(\n",
    "    (np.full_like(i1_n, Rload1_n), np.full_like(i2_n, Rload2_n), np.full_like(i3_n, Rload3_n)), axis=0\n",
    ")\n",
    "\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"Normalizer for the input data.\"\"\"\n",
    "    def __init__(self, X: np.ndarray):\n",
    "        i_mean, i_std, v_mean, v_std, dt_mean, dt_std = self._get_means(X)\n",
    "        # add dummy values for D\n",
    "        self.mean = np.array([i_mean, v_mean, i_mean, v_mean, 0.0, dt_mean])\n",
    "        self.std = np.array([i_std, v_std, i_std, v_std, 1, dt_std])\n",
    "        \n",
    "    \n",
    "    def _get_means(self, X: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "        # i_all is i_n with concatenated the LAST VALUE of i_np1\n",
    "        i_n_last = X[:, 0]  # i_n\n",
    "        i_np1_last = X[-1, 2]\n",
    "        i_all = np.concatenate((i_n_last, [i_np1_last]), axis=0)\n",
    "        i_mean = i_all.mean()\n",
    "        i_std = i_all.std()\n",
    "        \n",
    "        # v_all is v_n with concatenated the LAST VALUE of v_np1\n",
    "        v_n_last = X[:, 1]\n",
    "        v_np1_last = X[-1, 3]\n",
    "        v_all = np.concatenate((v_n_last, [v_np1_last]), axis=0)\n",
    "        v_mean = v_all.mean()\n",
    "        v_std = v_all.std()\n",
    "        \n",
    "        dt_mean = X[:, 5].mean()\n",
    "        dt_std = X[:, 5].std()\n",
    "        return i_mean, i_std, v_mean, v_std, dt_mean, dt_std\n",
    "\n",
    "\n",
    "    def normalize(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Normalize the input data X using the mean and std\n",
    "        X_norm = (X - self.mean) / self.std\n",
    "        return X_norm\n",
    "    \n",
    "    def denormalize(self, X_norm: np.ndarray) -> np.ndarray:\n",
    "        # Denormalize the input data X_norm using the mean and std\n",
    "        X_denorm = X_norm * self.std + self.mean\n",
    "        return X_denorm\n",
    "\n",
    "normalizer = Normalizer(X)\n",
    "X_norm = normalizer.normalize(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create simple feedforward neural network that estimates parameters\n",
    "class ParamEstimator(nn.Module):\n",
    "    \"\"\"Predicts physical parameters [L, RL, C, RC, Rdson, Rload, Vin, Vf] from a single sample.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_layers: List[int]):\n",
    "        super().__init__()\n",
    "        dims = [input_dim] + hidden_layers + [7]\n",
    "        layers: List[nn.Module] = []\n",
    "        for in_dim, out_dim in zip(dims[:-1], dims[1:]):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            # last layer linear, others tanh\n",
    "            if out_dim != 7:\n",
    "                layers.append(nn.Tanh())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [i_n, v_n, i_np1, v_np1, D, dt]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def denorm_physical_params(\n",
    "    L: torch.Tensor,\n",
    "    RL: torch.Tensor,\n",
    "    C: torch.Tensor,\n",
    "    RC: torch.Tensor,\n",
    "    Rdson: torch.Tensor,\n",
    "    Vin: torch.Tensor,\n",
    "    Vf: torch.Tensor,\n",
    ") -> Tuple[\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "    torch.Tensor,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Denormalize the physical parameters from their logarithmic form.\n",
    "    The parameters are expected to be in the logarithmic scale.\n",
    "    \"\"\"\n",
    "    L = torch.exp(L) * 1e-6  # assume the network gets the uH value\n",
    "    RL = torch.exp(RL)\n",
    "    C = torch.exp(C) * 1e-6  # assume the network gets the uF value\n",
    "    RC = torch.exp(RC)\n",
    "    Rdson = torch.exp(Rdson) * 1e-3  # the Rdson is quite small, so we assume it is in mOhm\n",
    "    Vin = torch.exp(Vin)*10  # Vin is in V\n",
    "    Vf = torch.exp(Vf)  # Vf is in V\n",
    "    return L, RL, C, RC, Rdson, Vin, Vf\n",
    "\n",
    "\n",
    "# --- Physics Forward RK4 ---\n",
    "def physics_forward(\n",
    "    x_n: torch.Tensor, params: torch.Tensor, normalizer: Normalizer, Rload: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given x_n = [i_n, v_n, i_np1, v_np1, D, dt] and predicted params,\n",
    "    reconstruct x_np1_pred = [i_np1, v_np1].\n",
    "    \"\"\"\n",
    "    # unnormalize the input data\n",
    "    x_n = normalizer.denormalize(x_n)\n",
    "    \n",
    "    # unpack inputs\n",
    "    i_n = x_n[:, 0:1]\n",
    "    v_n = x_n[:, 1:2]\n",
    "    D = x_n[:, 4:5]\n",
    "    dt = x_n[:, 5:6]\n",
    "    \n",
    "    # unpack params\n",
    "    L, RL, C, RC, Rdson, Vin, Vf = torch.split(params, 1, dim=1)\n",
    "\n",
    "    # the model actually predicts the logarithm of the parameters:denormalize parameters\n",
    "\n",
    "    L, RL, C, RC, Rdson, Vin, Vf = denorm_physical_params(\n",
    "        L, RL, C, RC, Rdson, Vin, Vf\n",
    "    )\n",
    "\n",
    "    i_np1_pred, v_np1_pred = predict_next_state(\n",
    "        i_n, v_n, D, dt, L, RL, C, RC, Rdson, Vin, Vf, Rload\n",
    "    )\n",
    "\n",
    "    return torch.cat([i_np1_pred, v_np1_pred], dim=1)\n",
    "\n",
    "\n",
    "def predict_next_state(\n",
    "    i_n: torch.Tensor,\n",
    "    v_n: torch.Tensor,\n",
    "    D: torch.Tensor,\n",
    "    dt: torch.Tensor,\n",
    "    L: torch.Tensor,\n",
    "    RL: torch.Tensor,\n",
    "    C: torch.Tensor,\n",
    "    RC: torch.Tensor,\n",
    "    Rdson: torch.Tensor,\n",
    "    Vin: torch.Tensor,\n",
    "    Vf: torch.Tensor,\n",
    "    Rload: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Predict the next state [i_np1, v_np1] using the RK4 method.\"\"\"\n",
    "\n",
    "    def f(i: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        di = -((D * Rdson + RL) * i + v - D * Vin + (1 - D) * Vf) / L\n",
    "        dv = (C * RC * Rload * di + Rload * i - v) / (C * (RC + Rload))\n",
    "        return di, dv\n",
    "\n",
    "    # RK4 steps\n",
    "    k1_i, k1_v = f(i_n, v_n)\n",
    "    k2_i, k2_v = f(i_n + 0.5 * dt * k1_i, v_n + 0.5 * dt * k1_v)\n",
    "    k3_i, k3_v = f(i_n + 0.5 * dt * k2_i, v_n + 0.5 * dt * k2_v)\n",
    "    k4_i, k4_v = f(i_n + dt * k3_i, v_n + dt * k3_v)\n",
    "\n",
    "    i_np1_pred = i_n + (dt / 6) * (k1_i + 2 * k2_i + 2 * k3_i + k4_i)\n",
    "    v_np1_pred = v_n + (dt / 6) * (k1_v + 2 * k2_v + 2 * k3_v + k4_v)\n",
    "    return i_np1_pred, v_np1_pred\n",
    "\n",
    "\n",
    "# --- Loss ---\n",
    "def compute_loss(x_np1_pred: torch.Tensor, x_np1_true: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum((x_np1_pred - x_np1_true) ** 2)\n",
    "\n",
    "\n",
    "def criterion(model, X, normalizer):\n",
    "    \"\"\"\n",
    "    Full-batch loss used by both Adam and L-BFGS.\n",
    "    \"\"\"\n",
    "    params_pred = model(X)  # (N, 8)\n",
    "    x_np1_pred = physics_forward(X, params_pred, normalizer)  # (N, 2)\n",
    "    x_np1_true = X[:, 2:4]  # (N, 2)  ->  [i_{n+1}, v_{n+1}]\n",
    "    return compute_loss(x_np1_pred, x_np1_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JC28LS\\AppData\\Local\\Temp\\ipykernel_35320\\1726381081.py:85: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  X_denorm = X_norm * self.std + self.mean\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m     17\u001b[39m params_pred = model(X_t)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m x_n1_pred = \u001b[43mphysics_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRload_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m loss = compute_loss(X_t[:, :\u001b[32m2\u001b[39m], x_n1_pred)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# backward propagation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mphysics_forward\u001b[39m\u001b[34m(x_n, params, normalizer, R_load)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# the model actually predicts the logarithm of the parameters:denormalize parameters\u001b[39;00m\n\u001b[32m     74\u001b[39m L, RL, C, RC, Rdson, Vin, Vf = denorm_physical_params(\n\u001b[32m     75\u001b[39m     L, RL, C, RC, Rdson, Vin, Vf\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m i_np1_pred, v_np1_pred = \u001b[43mpredict_next_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRdson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRload\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat([i_np1_pred, v_np1_pred], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mpredict_next_state\u001b[39m\u001b[34m(i_n, v_n, D, dt, L, RL, C, RC, Rdson, Vin, Vf, Rload)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m di, dv\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# RK4 steps\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m k1_i, k1_v = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m k2_i, k2_v = f(i_n + \u001b[32m0.5\u001b[39m * dt * k1_i, v_n + \u001b[32m0.5\u001b[39m * dt * k1_v)\n\u001b[32m    109\u001b[39m k3_i, k3_v = f(i_n + \u001b[32m0.5\u001b[39m * dt * k2_i, v_n + \u001b[32m0.5\u001b[39m * dt * k2_v)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mpredict_next_state.<locals>.f\u001b[39m\u001b[34m(i, v)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mf\u001b[39m(i: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    102\u001b[39m     di = -((D * Rdson + RL) * i + v - D * Vin + (\u001b[32m1\u001b[39m - D) * Vf) / L\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     dv = (\u001b[43mC\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mRC\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mRload\u001b[49m * di + Rload * i - v) / (C * (RC + Rload))\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m di, dv\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JC28LS\\AppData\\Local\\miniconda3\\envs\\EM+\\Lib\\site-packages\\torch\\_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model for 10 epochs\n",
    "\n",
    "X_t = torch.tensor(X_norm, dtype=torch.float32)\n",
    "Rload_t = torch.tensor(Rload, dtype=torch.float32).view(-1, 1)\n",
    "model = ParamEstimator(input_dim=6, hidden_layers=[32, 32])\n",
    "device = \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "X_t = X_t.to(device)\n",
    "Rload_t = Rload_t.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=adam_lr)\n",
    "\n",
    "for epoch in range(int(10e4)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass\n",
    "    params_pred = model(X_t)\n",
    "    x_n1_pred = physics_forward(X_t, params_pred, normalizer, Rload_t)\n",
    "    loss = compute_loss(X_t[:, :2], x_n1_pred)\n",
    "    \n",
    "    # backward propagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # get explicit predictions\n",
    "    L, RL, C, RC, Rdson, Vin, Vf = torch.split(params_pred, 1, dim=1)\n",
    "    \n",
    "    # denormalize parameters and print the predicted values\n",
    "    L, RL, C, RC, Rdson, Vin, Vf = denorm_physical_params(\n",
    "        L, RL, C, RC, Rdson, Vin, Vf\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"loss: {loss.item():.3e}, L: {L.mean().item():.6f} H, RL: {RL.mean().item():.6f} Ohm, C: {C.mean().item():.6f} F\")\n",
    "        print(f\"RC: {RC.mean().item():.6f} Ohm, Rdson: {Rdson.mean().item():.6f} Ohm, Vin: {Vin.mean().item():.6f} V, Vf: {Vf.mean().item():.6f} V\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4f860d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting L‑BFGS optimisation … (this may take a while)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "physics_forward() missing 1 required positional argument: 'R_load'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting L‑BFGS optimisation … (this may take a while)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mlbfgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mL‑BFGS finished.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JC28LS\\AppData\\Local\\miniconda3\\envs\\EM+\\Lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JC28LS\\AppData\\Local\\miniconda3\\envs\\EM+\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JC28LS\\AppData\\Local\\miniconda3\\envs\\EM+\\Lib\\site-packages\\torch\\optim\\lbfgs.py:330\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    327\u001b[39m state.setdefault(\u001b[33m\"\u001b[39m\u001b[33mn_iter\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m orig_loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m loss = \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[32m    332\u001b[39m current_evals = \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\JC28LS\\AppData\\Local\\miniconda3\\envs\\EM+\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mclosure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m():\n\u001b[32m      9\u001b[39m     lbfgs.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     loss.backward()\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mcriterion\u001b[39m\u001b[34m(model, X, normalizer)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mFull-batch loss used by both Adam and L-BFGS.\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    126\u001b[39m params_pred = model(X)  \u001b[38;5;66;03m# (N, 8)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m x_np1_pred = \u001b[43mphysics_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, 2)\u001b[39;00m\n\u001b[32m    128\u001b[39m x_np1_true = X[:, \u001b[32m2\u001b[39m:\u001b[32m4\u001b[39m]  \u001b[38;5;66;03m# (N, 2)  ->  [i_{n+1}, v_{n+1}]\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m compute_loss(x_np1_pred, x_np1_true)\n",
      "\u001b[31mTypeError\u001b[39m: physics_forward() missing 1 required positional argument: 'R_load'"
     ]
    }
   ],
   "source": [
    "lbfgs = torch.optim.LBFGS(\n",
    "    model.parameters(), \n",
    "    max_iter=lbfgs_epochs, \n",
    "    tolerance_grad=1e-9,\n",
    "    line_search_fn='strong_wolfe',\n",
    "    )\n",
    "\n",
    "def closure():\n",
    "    lbfgs.zero_grad()\n",
    "    loss = criterion(model, X_t, normalizer)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "print(\"Starting L‑BFGS optimisation … (this may take a while)\")\n",
    "lbfgs.step(closure)\n",
    "print(\"L‑BFGS finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb90bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final parameters after L-BFGS:\n",
      "L: 0.000022 H vs 0.0007250000000000001 H\n",
      "RL: 0.654444 Ohm vs 0.314 Ohm\n",
      "C: 0.000004 F vs 0.00016450000000000001 F\n",
      "RC: 2.213222 Ohm vs 0.20099999999999998 Ohm\n",
      "Rdson: 0.000524 Ohm vs 0.221 Ohm\n",
      "Rload: 1.169392 Ohm vs 3.1 Ohm\n",
      "Vin: 3.399695 V vs 48 V\n",
      "Vf: 0.001338 V vs 1.0 V\n"
     ]
    }
   ],
   "source": [
    "L, RL, C, RC, Rdson, Rload, Vin, Vf = torch.split(model(X_t), 1, dim=1)\n",
    "\n",
    "# Denormalize parameters\n",
    "L, RL, C, RC, Rdson, Rload, Vin, Vf = denorm_physical_params(\n",
    "    L, RL, C, RC, Rdson, Rload, Vin, Vf\n",
    ")\n",
    "print(\"Final parameters after L-BFGS:\")\n",
    "\n",
    "print(f\"L: {L.mean().item():.6f} H vs {L_n} H\")\n",
    "print(f\"RL: {RL.mean().item():.6f} Ohm vs {RL_n} Ohm\")\n",
    "print(f\"C: {C.mean().item():.6f} F vs {C_n} F\")\n",
    "print(f\"RC: {RC.mean().item():.6f} Ohm vs {RC_n} Ohm\")\n",
    "print(f\"Rdson: {Rdson.mean().item():.6f} Ohm vs {Rdson_n} Ohm\")\n",
    "print(f\"Rload: {Rload.mean().item():.6f} Ohm vs {Rload1} Ohm\")\n",
    "print(f\"Vin: {Vin.mean().item():.6f} V vs {Vin_n} V\")\n",
    "print(f\"Vf: {Vf.mean().item():.6f} V vs {VF_n} V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56048f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EM+",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
